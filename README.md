# Transformer network application: Question Answering (with Pytorch)

## Project description 
We want to build an application of the transformer architecture: Question Answering.
Question answering (QA) is a task of NLP that aims to automatically answer questions. The goal of extractive QA is to identify the portion of the text that contains the answer to a question. 
For example, QA aims at answering the question "When will Jane go to Africa?" based on the text "Jane visits Africa in September". The QA model will highlight "September". The goal of extractive QA is to find the part of the text that contains the answer to the question. We identify the position of the answer using the string indexes of the context. With this example, we need to find the start and end string index of the word "September" in the context sentence "Jane visits Africa in September.".
In this project we aim at performing extractive QA and implementing the QA model with Pytorch. We will use a pre-trained transformer model (DistilBertForQuestionAnswering) that we will fine-tuned.


## Dataset
We use the QA bAbI dataset, which is generated by Facebook AI Research to advance NLP tasks. This dataset already contains the train and test sets. Both train and test sets have 1000 records.
The records of each set are a nested dictionnary. The sub-dictionnary contains the following information:
* answer: a list of 3 elements with the last element capturing the answer
* id: a list of 3 indexes
* supporting_ids: the id of the sentence in the context where the answer comes from. We controlled that all inputs have the same structure of supporting_ids ([[], [], ['2']] or [[], [], ['1']], meaning either the 1st or 2nd sentence in the context)
* text: a list of 3 elements. The first 2 elements are the sentences which serve as context and the last element is the question
* type: a list of 3 indexes with 2 values: "0" tags the context sentence and "1" tags the question sentence. We controlled that all inputs have the same structure (type = [0, 0, 1]).

For example, for the question "What is east of the hallway?", the supporting fact "The bedroom is east of the hallway" has the id "2". The answer is "bedroom" from the 2nd sentence in the context.


### Data preprocessing
After loading the data, we flatten the data to remove the nested dictionnary to end up wth a simple dictionnary with same information. It makes the data easier to work with. Then, we extract the question and the context from the list variable text. We also extract the answer from the list variable answer.
Then, we extract the start and end index of the answer in the context. To do so, we look for the answer word in the context and extract its indexes. If we end up with several indexes (meaning, that the answer word appears in both sentences of the context), we take the one matching the supporting_ids (which tell us in which sentence of the context the answer appears). The end index is called exclusive, meaning it's the index of the character next to the answer.

We tokenize the context and the question, meaning we convert each word/subword into tokens (index). Indeed to feed text data to a Transformer model, input data must be tokenized. We use DistilBertTokenizerFast which is a fast implementation of the tokenizer for the DistilBERT model designed to quickly prepare text data for input into the model while supporting important preprocessing steps like padding and truncating. 
It handles several tasks:
* Splitting text into tokens
* Padding and truncating sequences to a fixed length
* Converting tokens back to text for easier interpretation
* Subword tokenization: it uses a technique called WordPiece (also used in BERT), where words are split into subwords to handle rare or unseen words
This tokenizer has the following characteristics:
* Transform words to lower case
* Vocabulary has 30522 words
* Tokenize chinese characters
* Standardize the length of the sequence to 512 by padding with 0 (on the right, if needed) and/or truncating (on the right, if needed)
* Use of special tokens:
    * 100: unknown token
    * 102: separate token
    * 0: pad token
    * 101: cls token (used to signal the start of a sequence)
    * 103: mask token
* Advantage to be fast: it uses tokenizers library, which is written in Rust (programming language that focuses on performance), to offer faster performance compared to the regular DistilBertTokenizer

Finally, we align the start and end indexes with the token(s) associated with the target answer word. Indeed, the tokenizer step may split a word into subwords, so multiple subtokens (for example "happyness" is split into 2 tokens "happy" [3407] and "##ness" [2791]). This also gives us a mask list (same length as the tokens), which is used during the attention mechanism to avoid focusing on padding tokens (ensuring the model does not "attend" to irrelevant parts of the sequence).


## Model
We implement the extractive QA model using PyTorch framework. Pytorch is an open source machine learning framework developed by Facebook's AI Research lab that can be used for computer vision and NLP tasks. 

For this project, we use the DistilBertForQuestionAnswering pre-trained model. DistilBertForQuestionAnswering is fine-tuned for QA tasks, using the DistilBERT architecture (more details about this model and the initial BERT model below). The model is fine-tuned on QA datasets to predict the start and end positions of an answer in a given context (passage of text), based on the input question. 
Input expected by this model:
* Context: a passage or paragraph containing the answer
* Question: a query that asks for information within the context
Output: the model predicts the start and end token indexes of the answer within the context.

To train the model, we use the Hugging Face Trainer, which contains a basic training loop and is compatible with Hugging Face models (easy to implement in PyTorch). The warmup phase increases the learning rate at the beginning of the training to ensure that the model stabilizes. The weight decay refers to a regularization (L2) technique. It helps prevent overfitting by penalizing large weights during training. It works by adding a term to the loss functoion that is proportional to the square of the magnitude of the model weight to encourage the model to keep the weights smaller, promoting simple models that genezlize better.

We use the F1 metric for this task. It's a classification metric, which handles well imbalanced dataset and aims at balancing precision and recall (more details below).

We make sure that both the model and input data are on the same device (either CPU or GPU). It is necessary for efficient computation.


### More details about BERT model
BERT (Bidirectional Encoder Representations from Transformers) is a model introduced by Google in 2018, which revolutionized NLP by using a transformer-based architecture. It was designed to pre-train a deep bidirectional representation by jointly conditioning on both left and right context in all layers, which was a significant improvement over previous models like GPT, which processes text unidirectionally (either left-to-right or right-to-left).
BERT is an encoder-only model, meaning it is designed only with an encoder, not a decoder (quick overview of that below).

The key innovation of the Transformer architecture is the self-attention mechanism, which allows the model to process sequences of data in parallel rather than sequentially (as with earlier models like RNNs and LSTMs). This makes Transformers faster and more efficient to train, and they achieve better performance on a variety of NLP tasks.

Key components of the Transformer architecture:
* Self-attention mechanism: it helps the model weigh the importance of each word in a sequence relative to all other words, regardless of their position. It allows the model to capture dependencies between words that are far apart in a sequence (which is hard for RNNs and LSTMs). It computes a score for each pair of words in the sequence, determining how much focus one word should have on another.
* Multi-head attention: in the Transformer, the self-attention mechanism is divided into multiple "heads" which allows the model to attend to different parts of the sentence simultaneously. This means that it can learn to focus on different relationships within the sentence at the same time.
* Positional encoding: since Transformers don't process text in a sequential manner like RNNs (which have memory), they need a way to understand the order of the words. This is where positional encodings come in. These encodings are added to the input embeddings to provide the model with information about the position of words in the sentence.
* Feedforward networks: after the self-attention operation, the output goes through a position-wise feedforward NN to process it further. This is done independently for each position (word) in the sequence.
* Layer normalization and residual connections: they help stabilize the training process by normalizing the outputs at each layer and maintaining gradients throughout the network

BERT operates in two phases: 
* Pre-training: during the pre-training phase, BERT is trained on a massive corpus of text (like the entire Wikipedia and BookCorpus). It is pre-trained using two tasks:
    * Masked Language Modeling (MLM): in MLM, some percentage of the input tokens are randomly masked (replaced with a special token [MASK]), and the model is trained to predict the original values of those masked tokens based on the context from both sides. This is what makes BERT bidirectional: it looks at the entire context of the sentence, not just the left or right context.
    * Next Sentence Prediction (NSP): in this task, the model is provided with pairs of sentences and is tasked with predicting whether the second sentence is the actual next sentence in the text. This helps BERT understand relationships between sentences, which is useful for tasks like question answering or sentence pair classification.
* Fine-tuning: after the pre-training, BERT can be fine-tuned for specific downstream tasks (like text classification, named entity recognition, question answering, etc). In fine-tuning, the pre-trained BERT model is adapted by training it on the task-specific dataset. Fine-tuning is typically much faster because the model has already learnt a lot of general language knowledge during pre-training. During fine-tuning, a task-specific layer (like a classifier for text classification or a span predictor for question answering) is added on top of the pre-trained BERT model.
The entire model (both the pre-trained part and the new task-specific layer) is trained on the task data.

Examples of application of BERT:
* Text classification: classifying texts into categories, such as spam detection, sentiment analysis, etc
* Named Entity Recognition (NER): identifying entities like names of people, places, or organizations in a text
* Question Answering (QA): extracting answers from text passages given a question
* Text summarization: generating a concise summary of a given text
* Sentence pair tasks: predicting if a sentence entails, contradicts, or is neutral with respect to another


### More details about DistilBERT
DistilBERT is a smaller, faster, and more efficient version of BERT, designed to retain much of BERT's performance while reducing its size and computational requirements.

Main advantages:
* Smaller model: DistilBERT reduces the size of BERT by about 60% (removes some layers) while retaining 97% of its language understanding abilities
* Faster: due to fewer parameters, it is faster to train and deploy, making it more efficient for real-world applications
* Same architecture: it keeps the original transformer architecture of BERT but uses knowledge distillation, a technique where a smaller model (DistilBERT) is trained to mimic the performance of a larger model (BERT)

This model (like BERT) can be used for tasks like text classification, named entity recognition (NER), question answering and sentiment analysis.


### More details about F1 score
The F1 score is used as a performance metric (for classification task, and especially on imbalanced data) to provide a balanced measure of a model's ability to correctly identify relevant information while minimizing false positives and false negatives.
The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both:
* Precision: measures how many of the answers predicted by the model are correct (ie the proportion of relevant answers among all answers that the model predicted). In QA scenarios, high precision means minimizing irrelevant answers.
* Recall: measures how many of the correct answers were actually predicted by the model (ie the proportion of relevant answers that the model identified). In QA scenarios, high recall means finding as many correct answers as possible.

Macro F1 treats all classes equally, regardless of how many samples each class has. It's useful when we care about performance across all classes, especially in imbalanced datasets, because it doesn't favor the majority class.

Let's explain more concretely and in details how it's used for this project. We compute a F1 score for start indexes and a F1 score for end indexes. For a F1 score, we compute first the recall and the precision for each class index, then we compute the F1 score for each class index, then we average the F1 without weight (as macro required).
Let's take an example to better understand. Assume we have the following start indexes vector:
* true_start=[10,2,2,10,2,2]
* pred_start=[2,2,2,10,2,2]
Here we aim at predicting F1 for start indexes. So we need to compute precision and recall for each class index (here we have 2 indexes: 10 and 2):
* Precision for class 10:
    * TP=1 (true value = predicted value)
    * FP=0 (predicted but not true)
    * Precision (class 10) = 1/(1 + 0) = 1
* Recall for class 10:
    * TP=1 (true value = predicted value)
    * FN=1 (true but not predicted)
    * Recall (class 10) = 1/(1 + 1) = 0.5
* Precision for class 2:
    * TP=4 (true value = predicted value)
    * FP=1 (predicted but not true)
    * Precision (class 2) = 4/(4 + 1) = 0.8
* Recall for class 2:
    * TP=4 (true value = predicted value)
    * FN=0 (true but not predicted)
    * Recall (class 2) = 4/(4 + 0) = 1
* F1 computations (formula is ):
    * F1 for class 10: 2 * 1 * 0.5/(1 + 0.5) = 0.67
    * F1 for class 2: 2 * 0.8 * 1/(0.8 + 1) = 0.89
    * F1 for start indexes = (0.67 + 0.89)/2 = 0.78

These are the formulas (for a specific class):
* Precision = TP/(TP+FP)
* Recall = TP/(TP+FN)
* F1 = 2 * Precision * Recall/(Precision + Recall)

In this project, the case relative to the end index being greater or equal than start index is not explicitely handled, but it will be grasped by the F1 score


### More details about the different types of Transformers architecture
#### Encoder-only models
These models use only the encoder part of the Transformer architecture. The encoder is responsible for processing and understanding the input sequence. They are great for understanding and representing text but cannot generate new text.

Characteristics of encoder-only models:
* Bidirectional: the encoder processes the entire input sequence at once, meaning it has access to both the left and right context for each word
* Discriminative tasks: these models are mostly used for tasks where understanding the relationships between words or phrases is important but not for generating new sequences

Examples of encoder-only models:
* BERT: BERT is an encoder-only model trained on masked language modeling (MLM) and next sentence prediction (NSP) tasks. It is designed for text understanding tasks. It is not used for text generation.
* RoBERTa (Robustly optimized BERT approach): a variant of BERT that improves performance by training on more data and removing some of the constraints of BERT (eg NSP).
* DistilBERT: a smaller, faster, and more efficient version of BERT that retains most of BERT's performance for understanding tasks


#### Decoder-only models
These models use only the decoder part of the Transformer architecture. The decoder is responsible for generating output based on the input and the previously generated tokens. They are designed specifically for generating new text based on a given input.

Characteristics:
* Unidirectional (left-to-right): the decoder generates tokens one-by-one and has access only to the previously generated tokens (left-to-right context). This is useful for text generation tasks.
* Generative tasks: these models are primarily used for tasks where new text needs to be generated based on some initial input or prompt.

Examples:
* GPT (Generative Pretrained Transformer): GPT models use only the decoder and are trained using causal language modeling. They are designed for text generation tasks, where the model predicts the next token in a sequence based on the previous ones.
* GPT-2 / GPT-3: these models are a continuation of GPT, trained on much larger datasets, and capable of generating coherent and contextually relevant text over long passages.


#### Encoder-decoder models
These models use both the encoder and decoder from the Transformer architecture. The encoder processes the input, and the decoder generates the output based on the encoded representation. They are designed for tasks that involve both understanding the input and generating output, such as translation and summarization.

Characteristics:
* Encoder-decoder structure: these models can handle tasks where there is an input and an output sequence, such as machine translation or summarization
* Seq2Seq (Sequence-to-Sequence): they are ideal for tasks where we need to convert one sequence into another, like translating a sentence from one language to another or generating a summary of a document

Examples:
* T5 (Text-to-Text Transfer Transformer): T5 uses both the encoder and decoder, trained on the text-to-text framework where every task is formulated as a text generation problem
* BART (Bidirectional and Auto-Regressive Transformers): BART combines both the encoder and decoder and is trained using denoising autoencoding
* MarianMT: a multilingual translation model built using the encoder-decoder architecture, trained for machine translation tasks


## Script explanation
Following are the main steps of this project:
1. Load and preprocess the data (extract the information about context, question and answer, then determine the start and end index of the answer in the context)
2. Tokenize the data (transform from words/subwords to index)
3. Align the tokenization (control start and end indexes after the tokenization)
4. Load the model and specify the training arguments
5. Train the model on training dataset
6. Evaluate the model on test dataset
7. Check the model with an example of our own


## References
This script is coming from the Deep Learning Specialization course. I enriched it to this new version.
